<pre><span class="filter_mathjaxloader_equation"><div class="text_to_html">{&quot;nbformat&quot;:4,&quot;nbformat_minor&quot;:0,&quot;metadata&quot;:{&quot;colab&quot;:{&quot;name&quot;:&quot;DL01_MultiLayerPerceptron_Student.ipynb&quot;,&quot;provenance&quot;:[],&quot;collapsed_sections&quot;:[&quot;JS8FL6y3TJDs&quot;,&quot;M5fwLDzBTL5C&quot;,&quot;fwVuXdUbTPYg&quot;,&quot;ip4VBzCbTW1J&quot;,&quot;Q3Z5lbaBTebb&quot;,&quot;Z91SiPaaLJKR&quot;,&quot;xyZNstVLKDO_&quot;,&quot;_W4RLqu9SeKy&quot;],&quot;toc_visible&quot;:true},&quot;kernelspec&quot;:{&quot;display_name&quot;:&quot;Python 3&quot;,&quot;language&quot;:&quot;python&quot;,&quot;name&quot;:&quot;python3&quot;},&quot;language_info&quot;:{&quot;codemirror_mode&quot;:{&quot;name&quot;:&quot;ipython&quot;,&quot;version&quot;:3},&quot;file_extension&quot;:&quot;.py&quot;,&quot;mimetype&quot;:&quot;text/x-python&quot;,&quot;name&quot;:&quot;python&quot;,&quot;nbconvert_exporter&quot;:&quot;python&quot;,&quot;pygments_lexer&quot;:&quot;ipython3&quot;,&quot;version&quot;:&quot;3.5.4&quot;},&quot;accelerator&quot;:&quot;GPU&quot;},&quot;cells&quot;:[{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;DDD5ho2RmyNy&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;# Correction TP1 : Multi-Layer Perceptron&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;JS8FL6y3TJDs&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;# **I- Definition**&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;M5fwLDzBTL5C&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***1) The neuron***&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;IzhFo0IINDnq&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;The basic unit composant of a Neural network is called a **neuron**, or a **node**.  \n&quot;,&quot;A neuron performs a very straightforward operation : It receives several inputs $x_i$ and associates a **weight** $w_i$ to all of them.  \n&quot;,&quot;An **activation function** $f()$ is then applied on the weighted sum of these inputs plus a **bias** $b$ :&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;B-xdSEkWMZO7&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;![neuron](<a href="https://docs.google.com/uc?export=download&amp;id=1TDb-6flI--ILsgG8xag0yhGBqgce_K2D)&quot" class="_blanktarget">https://docs.google.com/uc?export=download&amp;id=1TDb-6flI--ILsgG8xag0yhGBqgce_K2D)&quot</a>;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;uFX_MtIKPYbo&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;Consequently, for $N$ inputs, this neuron has $N+1$ parameters to be tuned, and the output of the neuron will be: <span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=%20y%20%3D%20f%20%5C%5Cleft%28%5C%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20w_i%20%5C%5Ctimes%20x_i%20%2B%20b%5C%5Cright%29" id="action_link6391f2bb1de381" class=""  title="TeX" ><img class="texrender" title=" y = f \\left(\\sum_{i=1}^{n} w_i \\times x_i + b\\right)" alt=" y = f \\left(\\sum_{i=1}^{n} w_i \\times x_i + b\\right)" src="https://lms.isae.fr/filter/tex/pix.php/927ad0ccf8514856ecd6a2b43ce0c7ed.gif" /></a></span><script type="math/tex"> y = f \\left(\\sum_{i=1}^{n} w_i \\times x_i + b\\right)</script></span>  \n&quot;,&quot;As we will see later, the Activation function is there to introduce non-linearity into the output of a neuron.  \n&quot;,&quot;Some classical Activation function are the **Sigmoid**: <span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=f%28x%29%20%3D%20%5C%5Cfrac%7B1%7D%7B1%20%2B%20e%5E%7B-x%7D%7D" id="action_link6391f2bb1de382" class=""  title="TeX" ><img class="texrender" title="f(x) = \\frac{1}{1 + e^{-x}}" alt="f(x) = \\frac{1}{1 + e^{-x}}" src="https://lms.isae.fr/filter/tex/pix.php/b2d739be19817e08370d47def6db9fea.gif" /></a></span><script type="math/tex">f(x) = \\frac{1}{1 + e^{-x}}</script></span>\n&quot;,&quot;\n&quot;,&quot;or the **Rectified Linear Unit** (ReLu) : <span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=f%28x%29%20%3D%20max%280%2C%20x%29" id="action_link6391f2bb1de383" class=""  title="TeX" ><img class="texrender" title="f(x) = max(0, x)" alt="f(x) = max(0, x)" src="https://lms.isae.fr/filter/tex/pix.php/c61a9c23c2e481c545f8ef029842d15b.gif" /></a></span><script type="math/tex">f(x) = max(0, x)</script></span>\n&quot;,&quot;\n&quot;,&quot;The following code displays these two standard activation functions, as well as their derivative. The cube of these derivatives are also displayed for information (see *I-3-a*, back propagation, for further details).&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;_NnTw-CMgZ1l&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;outputId&quot;:&quot;d5d3c140-51b8-48c7-8b37-8ac1264fde02&quot;,&quot;executionInfo&quot;:{&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1591036509344,&quot;user_tz&quot;:-120,&quot;elapsed&quot;:2090,&quot;user&quot;:{&quot;displayName&quot;:&quot;Bauerheim Michael&quot;,&quot;photoUrl&quot;:&quot;&quot;,&quot;userId&quot;:&quot;10235967470403378991&quot;}},&quot;colab&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:374}},&quot;source&quot;:[&quot;import numpy as np\n&quot;,&quot;from matplotlib import pyplot as plt\n&quot;,&quot;\n&quot;,&quot;err = 0.0001\n&quot;,&quot;\n&quot;,&quot;x = np.arange(100)/10-5\n&quot;,&quot;Sigmoid = 1/(1+np.exp(-x))\n&quot;,&quot;ReLu = np.maximum(0, x)\n&quot;,&quot;\n&quot;,&quot;Sigmoidprime = Sigmoid*(1-Sigmoid)\n&quot;,&quot;Sigmoidprime3 = Sigmoidprime**3\n&quot;,&quot;ReLuprime = np.maximum(0, x)/(np.abs(x)+err)\n&quot;,&quot;ReLuprime3 = ReLuprime**3\n&quot;,&quot;\n&quot;,&quot;plt.figure(figsize=(12, 6))\n&quot;,&quot;\n&quot;,&quot;plt.subplot(121)\n&quot;,&quot;plt.plot(x, Sigmoid, &#039;k-&#039;, label=&#039;Sigmoid&#039;)\n&quot;,&quot;plt.plot(x, Sigmoidprime, &#039;r--&#039;, label=&#039;Sigmoid prime&#039;)\n&quot;,&quot;plt.plot(x, Sigmoidprime3, &#039;g-.&#039;, label=&#039;(Sigmoid prime)^3&#039;)\n&quot;,&quot;plt.grid()\n&quot;,&quot;plt.legend();\n&quot;,&quot;\n&quot;,&quot;plt.subplot(122)\n&quot;,&quot;plt.plot(x, ReLu, &#039;k-&#039;, label=&#039;ReLu&#039;)\n&quot;,&quot;plt.plot(x, ReLuprime, &#039;r--&#039;, label=&#039;ReLu prime&#039;)\n&quot;,&quot;plt.plot(x, ReLuprime3, &#039;g-.&#039;, label=&#039;(ReLu prime)^3&#039;)\n&quot;,&quot;plt.grid()\n&quot;,&quot;plt.legend();&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[{&quot;output_type&quot;:&quot;display_data&quot;,&quot;data&quot;:{&quot;image/png&quot;:&quot;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\n&quot;,&quot;text/plain&quot;:[&quot;&lt;Figure size 864x432 with 2 Axes&gt;&quot;]},&quot;metadata&quot;:{&quot;tags&quot;:[],&quot;needs_background&quot;:&quot;light&quot;}}]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;fwVuXdUbTPYg&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***2) The multi-Layer Perceptron***&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;lYnBwPPFntQq&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;A **multi-layer perceptron (MLP)** is a feedforward network which consists of several layers of neurons, in which each layer is fully connected to the following one.  Each neuron in these layers will receive several inputs, and will compute the previous equation with their own set of weights and a bias. Here, fully connected layers are displayed, where neurons of the first layer are all connected to the second one. Consequently, for $P$ neurons with $N$ inputs (either the initial inputs, or ones from a previous layer), this layer will contain $(N+1)P$ parameters to be tuned.\n&quot;,&quot;\n&quot;,&quot;![neuron](<a href="https://docs.google.com/uc?export=download&amp;id=10N9GIQuCdHOJPIE0K5S3ySfe096J6RL1" class="_blanktarget">https://docs.google.com/uc?export=download&amp;id=10N9GIQuCdHOJPIE0K5S3ySfe096J6RL1</a>)\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;,&quot;**EXERCISE 1**\n&quot;,&quot;\n&quot;,&quot;**1a)** How many parameters are contained in the hidden layer of the fully connected network displayed? How many for the output layer? \n&quot;,&quot;\n&quot;,&quot;Solution of **1a)** is:\n&quot;,&quot;...\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;ip4VBzCbTW1J&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***3) Training a Multi-Layer Perceptron***&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;xlGUEspROnTj&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;### ***a) Gradient Descent (GD)***\n&quot;,&quot;\n&quot;,&quot;The process by which a Multi-Layer Perceptron is trained is called the **gradient descent**, which consists in the minimization of a cost function $ \\mathcal{L}()$ depending on the model&#039;s parameters. For a parameter $p$ of the network, the gradient $\\frac{\\partial \\mathcal{L}}{\\partial p}$ is computed. The parameter $p$ is then updated in the direction of the gradient with a relaxation term $\\alpha$ (called the **learning rate**), so that \n&quot;,&quot;\n&quot;,&quot;<span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=%20p_%7Bnew%7D%20%3D%20p%20-%20%5C%5Calpha%20%5C%5Cfrac%7B%5C%5Cpartial%20%5C%5Cmathcal%7BL%7D%7D%7B%5C%5Cpartial%20p%7D%20" id="action_link6391f2bb1de384" class=""  title="TeX" ><img class="texrender" title=" p_{new} = p - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial p} " alt=" p_{new} = p - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial p} " src="https://lms.isae.fr/filter/tex/pix.php/ad1cd85ae806b7b47170a0a26727627a.gif" /></a></span><script type="math/tex"> p_{new} = p - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial p} </script></span>\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;,&quot;**EXERCISE 2**\n&quot;,&quot;\n&quot;,&quot;The **learning rate** is critical in machine learning, because controlling the **gradient descent**, i.e. the search of the **optimal parameters** $\\theta^*$ minimizing the cost function $\\mathcal{L}(\\theta)$. To gain experience on how the learning rate affects results, a synthetic cost function depending on two parameters $p_1$ and $p_2$ is considered:\n&quot;,&quot;\n&quot;,&quot;<span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=%5C%5Cmathcal%7BL%7D%28p_1%2C%20p_2%29%20%3D%20sin%5C%5Cleft%28p_1%5E2%20%2B%20%5Bp_2%2F1.2%5D%5E2%5C%5Cright%29%20%20-%200.5e%5E%7B-%5B%28p_1%2B2%29%5E2%20%2B%202%28p_2-1.5%29%5E2%5D%20%7D%20-%202e%5E%7B-%5B%28p_1%2B0.5%29%5E2%20%2B%28p_2%2B0.5.5%29%5E2%5D%20%7D%20" id="action_link6391f2bb1de385" class=""  title="TeX" ><img class="texrender" title="\\mathcal{L}(p_1, p_2) = sin\\left(p_1^2 + [p_2/1.2]^2\\right)  - 0.5e^{-[(p_1+2)^2 + 2(p_2-1.5)^2] } - 2e^{-[(p_1+0.5)^2 +(p_2+0.5.5)^2] } " alt="\\mathcal{L}(p_1, p_2) = sin\\left(p_1^2 + [p_2/1.2]^2\\right)  - 0.5e^{-[(p_1+2)^2 + 2(p_2-1.5)^2] } - 2e^{-[(p_1+0.5)^2 +(p_2+0.5.5)^2] } " src="https://lms.isae.fr/filter/tex/pix.php/036783fc806a2b6df54362b42d1b944a.gif" /></a></span><script type="math/tex">\\mathcal{L}(p_1, p_2) = sin\\left(p_1^2 + [p_2/1.2]^2\\right)  - 0.5e^{-[(p_1+2)^2 + 2(p_2-1.5)^2] } - 2e^{-[(p_1+0.5)^2 +(p_2+0.5.5)^2] } </script></span>\n&quot;,&quot;\n&quot;,&quot;Gradients of this function are known, but not explicitely given here. The function $\\mathcal{L}$ and its gradients $\\frac{\\partial \\mathcal{L}}{\\partial p_i}$ are hard coded in the following program. The cost function is also displayed.\n&quot;,&quot;\n&quot;,&quot;A **gradient descent** is coded, for which three parameters can be changed:\n&quot;,&quot;- The number of iteration `Niter` of the gradient descent\n&quot;,&quot;- The initial point in the 2D parameter space `(P1gd, P2gd)`. Note that the gradient descient algorithm is just updating these two values, according to the previous formula.\n&quot;,&quot;- The learning `lr` controlling the parameters&#039; update\n&quot;,&quot;\n&quot;,&quot;**2a)** Let&#039;s start with `Niter = 0` to avoid GD, and only display the function. Just by looking at the figure, what are the optimal parameters $p_1^*$ and $p_2^*$? Why a gradient descent would be needed?\n&quot;,&quot;\n&quot;,&quot;Solution of **2a)** is: ...\n&quot;,&quot;\n&quot;,&quot;**2b)** Now, the gradient descent will be employed to search for the optimal parameter automatically. Set the GD parameters so that `Niter = 40`, `P1gd = 0`, `P2gd = 1.5` and `lr = 0.1`. What are the optimal parameters? How do you describe this search? What could you propose to accelerate this optimization?\n&quot;,&quot;\n&quot;,&quot;Solution of **2b)** is: ...\n&quot;,&quot;\n&quot;,&quot;**2c)** Apply your solution by setting this parameter value to `0.3`. What do you conclude? Further increase this value to `0.5`. What happens now? Same question if further increased to `0.8`?\n&quot;,&quot;\n&quot;,&quot;Solution of **2c)** is: ...\n&quot;,&quot;\n&quot;,&quot;**2d)** Set back the parameters to `Niter = 60` and `lr = 0.1`, but now initialize the search in a different location of the parameters space, for instance `P1gd = -0.5`, `P2gd = 1.5`. What happens? Propose ideas to fix this problem (it is not asked to implement these solutions).\n&quot;,&quot;\n&quot;,&quot;Solution of **2d)** is: ...\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;,&quot;\n&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;yq3wSITh4JjC&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;import numpy as np\n&quot;,&quot;from matplotlib import pyplot as plt\n&quot;,&quot;from random import uniform\n&quot;,&quot;%matplotlib inline\n&quot;,&quot;\n&quot;,&quot;# 2D model or parameter p1 and p2\n&quot;,&quot;N = 100\n&quot;,&quot;x = 5*(np.linspace(0, 1, N)-0.5)\n&quot;,&quot;p1, p2 = np.meshgrid(x, x)\n&quot;,&quot;# Loss function depending on the parameter&#039;s model\n&quot;,&quot;Loss = np.sin(p1**2 + (p2/1.2)**2)  - 0.5*np.exp(-((p1+2)**2 + (p2-1.5)**2)/0.5) - 2*np.exp(-(p1+0.5)**2 - (p2+0.5)**2)\n&quot;,&quot;Lmin = np.amin(Loss) # Minimum of the loss function\n&quot;,&quot;\n&quot;,&quot;# Display the loss function\n&quot;,&quot;from pylab import meshgrid,cm,imshow,contour,clabel,colorbar,axis,title,show\n&quot;,&quot;fig, ax = plt.subplots(figsize=(16, 8))\n&quot;,&quot;ax1=plt.subplot(1, 2, 1)\n&quot;,&quot;im = imshow(Loss,cmap=cm.hot, interpolation=\&quot;bicubic\&quot;, origin=\&quot;lower\&quot;, extent=[-2.5,2.5,-2.5,2.5]) # drawing the function\n&quot;,&quot;# adding the Contour lines with labels\n&quot;,&quot;cset = contour(p1, p2, Loss,np.arange(-2,2,0.25),linewidths=2,cmap=cm.gray)\n&quot;,&quot;clabel(cset,inline=True,fmt=&#039;%1.1f&#039;,fontsize=10)\n&quot;,&quot;#colorbar(im) # adding the colobar on the right\n&quot;,&quot;# latex fashion title\n&quot;,&quot;title(&#039;$\\mathcal{L}(p_1, p_2)$&#039;)\n&quot;,&quot;\n&quot;,&quot;# Choose an initial point\n&quot;,&quot;P1gd = 0.0\n&quot;,&quot;P2gd = 1.5\n&quot;,&quot;ax1.plot(P1gd,P2gd, &#039;k&#039;, marker=&#039;o&#039;,  lw = 2, markersize = 10, markerfacecolor = &#039;black&#039;)\n&quot;,&quot;\n&quot;,&quot;# Gradient descent\n&quot;,&quot;Niter = 0\n&quot;,&quot;lr = 0.1\n&quot;,&quot;LossEval = np.zeros(Niter)\n&quot;,&quot;for k in range(Niter):\n&quot;,&quot;  # Gradient computation\n&quot;,&quot;  dLdP1 = 2*P1gd*np.cos(P1gd**2+(P2gd/1.2)**2) -(-(2+P1gd)*np.exp(-((P1gd+2)**2+(P2gd-1.5)**2)/0.5))/(0.5) +4*(P1gd+0.5)*np.exp(-(P1gd+0.5)**2-(P2gd+0.5)**2)\n&quot;,&quot;  dLdP2 = (2*P2gd*np.cos(P1gd**2+(P2gd/1.2)**2))/(1.2**2) -(-(-1.5+P2gd)*np.exp(-((P1gd+2)**2+(P2gd-1.5)**2)/0.5))/(0.5) +4*(P2gd+0.5)*np.exp(-(P1gd+0.5)**2-(P2gd+0.5)**2)\n&quot;,&quot;\n&quot;,&quot;  # Parameter update\n&quot;,&quot;  P1gd = P1gd - lr*dLdP1\n&quot;,&quot;  P2gd = P2gd - lr*dLdP2\n&quot;,&quot;  # Plot the descent\n&quot;,&quot;  ax1.plot(P1gd,P2gd, &#039;k&#039;, marker=&#039;o&#039;,  lw = 2, markersize = 10, markerfacecolor = &#039;white&#039;)\n&quot;,&quot;\n&quot;,&quot;  # Loss evaluation\n&quot;,&quot;  LossEval[k] = np.sin(P1gd**2 + (P2gd/1.2)**2)  - 0.5*np.exp(-((P1gd+2)**2 + (P2gd-1.5)**2)/0.5) - 2*np.exp(-(P1gd+0.5)**2 - (P2gd+0.5)**2)\n&quot;,&quot;\n&quot;,&quot;  # Print data\n&quot;,&quot;  kiter=k+1\n&quot;,&quot;  print(&#039;iter = %1.0f, P1gd = %1.3f, P2gd = %1.3f and the Loss is L = %1.5f&#039; % (kiter, P1gd, P2gd, LossEval[k]))\n&quot;,&quot;\n&quot;,&quot;ax2=plt.subplot(1, 2, 2)\n&quot;,&quot;ax2.plot(LossEval, &#039;k-&#039;, marker=&#039;o&#039;)\n&quot;,&quot;ax2.plot([0, Niter], [Lmin, Lmin], &#039;r--&#039;)\n&quot;,&quot;\n&quot;,&quot;plt.show()\n&quot;,&quot;\n&quot;,&quot;\n&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;Q2T5Ufh4EEq2&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;Thus, this simple example highlighted the difficulty of training neural networks, which falls in the very large field of **non-convex optimization**. Note that in the previous example, the loss function was explicitely given and 2D. For neural networks, the loss landscape is of course not known, and very high-dimensional ($~\\gt 10^5$ in most deep neural networks). Literature has produced multiple aglorithms (SGD, RMSprop, Adam etc.) and training techniques (decaying and cyclic learning rate etc.), not detailed in this lecture. However, almost all training strategies for deep neural networks are based on gradient-based optimisation, which rises another major difficulty when training such models.&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;H2yrvnO6WN-S&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;### ***b) Back propagation*** \n&quot;,&quot;\n&quot;,&quot;Another difficulty in neural networks is therefore to compute all the derivatives $\\frac{\\partial \\mathcal{L}}{\\partial p}$ in an efficient way, in particular when the number of parameters $p$ is very high (for instance, VGG16, a well known deep learning network for classification, contains $138$ millions of parameters to be tuned). This is achieved using the [BackPropagation](<a href="https://www.nature.com/articles/323533a0" class="_blanktarget">https://www.nature.com/articles/323533a0</a>) algorithm.  \n&quot;,&quot;\n&quot;,&quot;Thus for **supervised learning**, for every samples in the training set $\\{x, y\\}$, the output of the MLP is computed and compared to the true value $\\{x, y^t\\}$, often called **ground truth** or **target**. A measure of this error is provided by the **loss function**. The definition of this loss function $ \\mathcal{L}()$ is crucial in a machine learning problem, and depends strongly on the problem (regression, classification etc.). A typical example for regression problem is the **Mean Square error (MSE)**, defined as\n&quot;,&quot;\n&quot;,&quot;<span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=%20%5C%5Cmathcal%7BL%7D%28y%2C%20y_t%29%20%3D%20%5C%5Cfrac%7B1%7D%7BN%7D%5C%5Csum_%7Bi%3D1%7D%5EN%20%7Cy_i%20-%20y_i%5Et%7C%5E2%20" id="action_link6391f2bb1de386" class=""  title="TeX" ><img class="texrender" title=" \\mathcal{L}(y, y_t) = \\frac{1}{N}\\sum_{i=1}^N |y_i - y_i^t|^2 " alt=" \\mathcal{L}(y, y_t) = \\frac{1}{N}\\sum_{i=1}^N |y_i - y_i^t|^2 " src="https://lms.isae.fr/filter/tex/pix.php/66e6a6ddca849c24cd663392eefd1387.gif" /></a></span><script type="math/tex"> \\mathcal{L}(y, y_t) = \\frac{1}{N}\\sum_{i=1}^N |y_i - y_i^t|^2 </script></span>\n&quot;,&quot;\n&quot;,&quot;The loss function measuring this error is then propagated through the whole network : the gradient of this loss in respect of each weight is computed with the [chain rule](<a href="https://en.wikipedia.org/wiki/Chain_rule" class="_blanktarget">https://en.wikipedia.org/wiki/Chain_rule</a>) and every weights are updated accordingly. As a result, the **parameters of the neural network are optimized in order to minimize the error compared with target outputs**.\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;**EXERCISE 3**\n&quot;,&quot;\n&quot;,&quot;In this example, a simple 2-layer MLP with only one input $x$ and output $y$ is considered. The hidden state is denoted $h$, and the weight and bias of the $i$-th neuron are $w_i$ and $b_i$ respectively. The loss function is the MSE, which here simplifies as $\\mathcal{L}(y-y^t) = (y-y^t)^2$ since $y$ is a single scalar value. Let&#039;s note $z_i$ the quantity produced by the neuron just before the activation function, so that the $i$-th neuron is outputting the value $f_i(z_i)$.\n&quot;,&quot;\n&quot;,&quot;![Network-2layer](<a href="https://drive.google.com/uc?id=1vvzVosTkMdeF1R6BS_9jALHdTcjrbHli&amp;export=download" class="_blanktarget">https://drive.google.com/uc?id=1vvzVosTkMdeF1R6BS_9jALHdTcjrbHli&amp;export=download</a>)\n&quot;,&quot;\n&quot;,&quot;**3a)** Write the feedforward equation of the first neuron.\n&quot;,&quot;\n&quot;,&quot;Solution of **3a)** is: ...\n&quot;,&quot;\n&quot;,&quot;**3b)** Write the feedforward equation for the second neuron\n&quot;,&quot;\n&quot;,&quot;Solution of **3b)** is: ...\n&quot;,&quot;\n&quot;,&quot;**3c)** Write the derivative of the loss function with respect to the output $y$:\n&quot;,&quot;\n&quot;,&quot;Solution of **3c)** is: ...\n&quot;,&quot;\n&quot;,&quot;**3d)** Write the derivative of the loss $\\mathcal{L}$ with respect to the parameters of the **second** layer.\n&quot;,&quot;\n&quot;,&quot;Solution of **3d)** is: ...\n&quot;,&quot;\n&quot;,&quot;**3e)** Write the derivative of the loss $\\mathcal{L}$ with respect to the parameters of the **first** layer.\n&quot;,&quot;\n&quot;,&quot;Solution of **3e)** is: ...\n&quot;,&quot;\n&quot;,&quot;**3f)** What do you conclude? What could happen if more neurons were connected one after the other?\n&quot;,&quot;\n&quot;,&quot;Solution of **3f)** is: ...\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;DeT3D5ckkqeh&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;### ***c) Batch gradient descent***\n&quot;,&quot;\n&quot;,&quot;Note that the gradients of the loss function obtained in the previous example depend on the input $x$ and output target $y^t$. Consequently, when looking at several samples from the training dataset, various directions of the gradient will be produced. If the update of the model&#039;s parameters is performed for each sample, it might lead to a noisy descent. The best solution would be to average the descent directions over the whole dataset, yet it is usually impossible in practice because of memory issues. Thus, a **batch gradient descent** is often prefered. \n&quot;,&quot;\n&quot;,&quot;A **batch** is a collection of a small number of samples. The gradient descent will be performed only after the batch is evaluated, averaging the descent over the samples of the batch. The **batch size** is the number of samples used to approximate the gradient: a low batch size will produce a noisy optimization process, whereas a large batch size will create memory issues. In practice, the largest batch size allowed by the GPU memory is chosen.\n&quot;,&quot;\n&quot;,&quot;During the training phase, the learning algorithm will operate batch after batch until the whole dataset has been covered. This step is called an **epoch**. Then, the optimization is continue several times: many epochs are performed, meaning that the whole dataset is reused several times during the optimization process. It is common to see in the literature models trained for a large number of epochs, going from 10 to 1000, or even larger. Of course, this number strongly depends on, among others:\n&quot;,&quot;\n&quot;,&quot;- the number of samples in the training dataset\n&quot;,&quot;- the size of the neural network, i.e. the number of parameters to be tuned\n&quot;,&quot;- the intrinsic difficulty of the problem\n&quot;,&quot;- the desired accuracy level expected from the network (more accurate often requires more epochs).&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;Q3Z5lbaBTebb&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;# **II - TP with playground**&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;obzigS4TZz6N&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;Now that the fundamentals of a Multi-Layer Perceptron are introduced, let&#039;s try to get a handle on how to train them, and how they react to different datasets and hyperparameters with the website [playground.tensorflow.org](<a href="https://playground.tensorflow.org" class="_blanktarget">https://playground.tensorflow.org</a>).&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;Z91SiPaaLJKR&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***1) Gradient Vanishing***&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;3xk_IP1ymy-x&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;In this part, we will work on the **circular dataset** of Playground, without noise on the data (noise = 0). Assign the following values to the hyperparameters and options offered by the interface :\n&quot;,&quot;- **Learning Rate** = 0.03\n&quot;,&quot;- **Activation** = Sigmoid\n&quot;,&quot;- **Regularization** = None\n&quot;,&quot;- **Batch Size** = 20 (20 samples used per gradient update)\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;,&quot;**Exercise 4**\n&quot;,&quot;\n&quot;,&quot;**4a)** Start by using a **single hidden layer**, with **2 neurons**, and look at the resulting solution after few epochs. The capacity of the model seems too low for this problem.   Vary the number of neurons on the hidden layer, and observe the evolution of the separator. Observe also the synaptic weights: some should tend towards 0  \n&quot;,&quot;\n&quot;,&quot;Solution of **4a)** is: ...\n&quot;,&quot;\n&quot;,&quot;**4b)** Add a **second hidden layer**. What&#039;s going on with the convergence speed of the network now ?  Try to increase the learning rate to counterbalance this effect. What is the optimal learning rate here? \n&quot;,&quot;\n&quot;,&quot;Solution of **4b)** is: ...\n&quot;,&quot;\n&quot;,&quot;**4c)** Add a **third hidden layer**. What is happening now ?  Can you still find a Learning Rate that still achieves convergence ? \n&quot;,&quot;\n&quot;,&quot;Solution of **4c)** is: ...\n&quot;,&quot;\n&quot;,&quot;**4d)** Replace the sigmoid activation function with a **reLU function**. What is happening ?  \n&quot;,&quot;\n&quot;,&quot;Solution of **4d)** is: ...\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;xyZNstVLKDO_&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***2) Regularization***&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;UPeSB4E0Lfhr&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;The effect of the **regularization** is to add a penalty to the loss function equal to the sum of the absolute values of the weight for the L1 regularization (similar to **Lasso**), or the sum of the squared weights for the L2 (similar to **Ridge**). In this section, we will try to grasp the idea of how regularization works to prevent overfitting. We will work again on the circular data, but with some noise on the data (**noise = 50**).\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;,&quot;**Exercise 5**\n&quot;,&quot;\n&quot;,&quot;**5a)** Start by setting up a high-capacity network (depth = 7, with 6 hidden layers plus the last one) with 8 neurons per layer, the maximum in playground and a ReLu activation with a learning rate 0.1. You should gradually observe all signs of **overfitting** when the network starts to **memorize** the data in the training set and performs poorly in the test set.  \n&quot;,&quot;\n&quot;,&quot;**5b)** Add a regularization term L2. Try to establish which interval of values is efficient to reduce the overfitting. Observe the evolution of synaptic weights as the number of epochs is increasing. \n&quot;,&quot;\n&quot;,&quot;Solution of **5b)** is: ...\n&quot;,&quot;\n&quot;,&quot;**5c)** Same question with a L1 regularization term. \n&quot;,&quot;\n&quot;,&quot;Solution of **5c)** is: ...\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;_W4RLqu9SeKy&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***3) Defining a new network: architecture &amp; feature engineering***&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;Zn2yf-CxSqin&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;Now, let&#039;s try to define a network as compact as possible to solve new classification problems. To do so, **feature engineering** is of crucial interest. Even if deep neural network can approximate theoretically any function, a proper architecture and a right choice of inputs can reduce the network size, reduce the training phase and GPU requirements, as well as accelerate the inference time. Here are some examples to gain experience with building neural networks:\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;,&quot;**Exercise 6**\n&quot;,&quot;\n&quot;,&quot;**6a)** On the previous example, what could you propose for the input choice to minimize the network size?\n&quot;,&quot;\n&quot;,&quot;Solution of **6a)** is: ... \n&quot;,&quot;\n&quot;,&quot;**6b)** Start with the **checkerboard dataset**, with a noise of 10. Does adding features help for this task ?  \n&quot;,&quot;\n&quot;,&quot;Solution of **6b)** is: ...\n&quot;,&quot;\n&quot;,&quot;**6c)** Same question for the **vortex dataset**.  \n&quot;,&quot;\n&quot;,&quot;Solution of **6c)** is: ...\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;4D5Tmtk0KDTC&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;# **III - TP with PyTorch/Lightning**&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;iAoaQM9VMmAD&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***1) Install***\n&quot;,&quot;\n&quot;,&quot;We are now going to write our first neural network using **PyTorch** open source library. In python, it is loaded using\n&quot;,&quot;```\n&quot;,&quot;import torch\n&quot;,&quot;```\n&quot;,&quot;In order to reduce the amount of boilerplate code, the **PyTorch Lightning** is used. This library is a high-level interface for PyTorch that automates the parts of the code that are repeated in every DL project. This choice allows us to focus on the important concepts of Neural Network training and evaluation. This framework is getting more and more popular, and now is **the first framework used in research** (about $60\\%$ to $80\\%$ of the papers are produced with Pytorch in the main AI conferences)\n&quot;,&quot;\n&quot;,&quot;Keep in mind that other Deep Learning libraries are available, in particular **Tensorflow** and its high-level counterpart **Keras** (see the excellent book from Francois Chollet for further details on Keras). In particular, TensorFlow is still the **main framework used in industry**.\n&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;OR2DxhYWdQ8J&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;First, let&#039;s install PyTorch Lightning (**You may have to restart the colab runtime when prompted by the installation**) and the custom library for this TP1 (taken from gitlab repository <a href="https://gitlab.com/a.alguacil-cabrerizo/machine_learning_isae_1a)&quot" class="_blanktarget">https://gitlab.com/a.alguacil-cabrerizo/machine_learning_isae_1a)&quot</a>;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;VzaQMDIrj4pc&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;! pip install git+<a href="https://github.com/PytorchLightning/pytorch-lightning.git@master" class="_blanktarget">https://github.com/PytorchLightning/pytorch-lightning.git@master</a> --upgrade&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;FLbKEjhP1ZiJ&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;! FOLDER=\&quot;machine_learning_isae_1a\&quot;; URL=\&quot;<a href="https://gitlab.com/a.alguacil-cabrerizo/machine_learning_isae_1a" class="_blanktarget">https://gitlab.com/a.alguacil-cabrerizo/machine_learning_isae_1a</a>\&quot;; if [ ! -d \&quot;$FOLDER\&quot; ] ; then git clone $URL $FOLDER ; else cd \&quot;$FOLDER\&quot; &amp;&amp; git pull; fi&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;OrN1RcBlE2He&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;The next code snippet just loads the custom library as a python module.&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;NwAKbUOeq9YK&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;import importlib\n&quot;,&quot;import os\n&quot;,&quot;PROJECT_PATH = \&quot;machine_learning_isae_1a/dl/tp1\&quot;\n&quot;,&quot;\n&quot;,&quot;spec = importlib.util.spec_from_file_location(&#039;tp1&#039;, os.path.join(PROJECT_PATH, &#039;mlp.py&#039;))\n&quot;,&quot;tp1 = importlib.util.module_from_spec(spec)\n&quot;,&quot;spec.loader.exec_module(tp1)&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;FyeHeAdVbfVe&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***2) Data generation***\n&quot;,&quot;\n&quot;,&quot;Let&#039;s create a function to generate data similar to the playground circular dataset. As a reminder, points are randomly sampled in a 2D field. A rule has been set to paint those points, either in blue or red, with some noise. The goal is to find a surrogate model to classify these points as blue/red. Note that here the `seed()` is fixed to avoid that such a random dataset will vary everytime one is launching the program.&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;o9kpOoxkGl0y&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;import numpy as np\n&quot;,&quot;def generateData<img class="icon emoticon" alt="Non" title="Non" src="https://lms.isae.fr/theme/image.php/fordson/core/1669649644/s/no" />:\n&quot;,&quot;  a = np.random.rand<img class="icon emoticon" alt="Non" title="Non" src="https://lms.isae.fr/theme/image.php/fordson/core/1669649644/s/no" /> * 2 * np.pi\n&quot;,&quot;  c1 = 1*np.random.normal(0, 1, n)*np.vstack((np.cos(a), np.sin(a)))\n&quot;,&quot;\n&quot;,&quot;  b = np.random.rand<img class="icon emoticon" alt="Non" title="Non" src="https://lms.isae.fr/theme/image.php/fordson/core/1669649644/s/no" /> * 2 * np.pi\n&quot;,&quot;  c2 = (3.5+1*np.random.normal(0, 0.7, n))*np.vstack((np.cos(b), np.sin(b)))\n&quot;,&quot;\n&quot;,&quot;  x = np.concatenate((c1,c2), axis=1)\n&quot;,&quot;  y = np.concatenate((np.zeros(<img class="icon emoticon" alt="Non" title="Non" src="https://lms.isae.fr/theme/image.php/fordson/core/1669649644/s/no" />),np.ones(<img class="icon emoticon" alt="Non" title="Non" src="https://lms.isae.fr/theme/image.php/fordson/core/1669649644/s/no" />)),axis=0)\n&quot;,&quot;  \n&quot;,&quot;  return x, y\n&quot;,&quot;\n&quot;,&quot;from matplotlib import pyplot as plt\n&quot;,&quot;\n&quot;,&quot;np.random.seed(42)\n&quot;,&quot;x, y = generateData(200)\n&quot;,&quot;\n&quot;,&quot;plt.figure(figsize=(8, 8))\n&quot;,&quot;\n&quot;,&quot;plt.scatter(x[0][np.where(y == 0)], x[1][np.where(y == 0)], s=80, color=&#039;b&#039;)\n&quot;,&quot;plt.scatter(x[0][np.where(y == 1)], x[1][np.where(y == 1)], s=80, color=&#039;r&#039;)\n&quot;,&quot;\n&quot;,&quot;plt.axis(&#039;equal&#039;)\n&quot;,&quot;plt.grid()&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;x-tfpveh7_by&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;**EXERCISE 7**\n&quot;,&quot;\n&quot;,&quot;Before doing any machine learning, getting a thorough understanding of the current problem is crucial. Here, without machine, let&#039;s try to evaluate intuitive estimators:\n&quot;,&quot;\n&quot;,&quot;**7a)** Considering this problem as a general binary classification, what would be the minimal accuracy that a method should reach?\n&quot;,&quot;\n&quot;,&quot;Solution of **7a)** is: ...\n&quot;,&quot;\n&quot;,&quot;**7b)** It is always a good practice to first look at data before doing any ML. Here, looking at the sample distribution, propose a simple estimator. Evaluate its performance using the code below.\n&quot;,&quot;\n&quot;,&quot;Solution of **7b)** is: ...&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;_edreK8u96zJ&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;# Write your criterion here : r = ...\n&quot;,&quot;r = ...\n&quot;,&quot;Rcrit = ...\n&quot;,&quot;\n&quot;,&quot;# This code automatically compute your estimator (like this : r &lt; Rcrit), and display results\n&quot;,&quot;fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18,6))\n&quot;,&quot;\n&quot;,&quot;ax1.scatter(x[0][np.where(y == 1)], x[1][np.where(y == 1)], s=80, color=&#039;r&#039;)\n&quot;,&quot;ax1.scatter(x[0][np.where(y == 0)], x[1][np.where(y == 0)], s=80, color=&#039;b&#039;)\n&quot;,&quot;ax1.axis(&#039;equal&#039;)\n&quot;,&quot;ax1.grid()\n&quot;,&quot;ax1.set_ylim([-6, 6])\n&quot;,&quot;ax1.set_xlim([-6, 6])\n&quot;,&quot;\n&quot;,&quot;ax2.scatter(x[0][np.where(r &gt; Rcrit)], x[1][np.where(r &gt; Rcrit)], s=80, color=&#039;r&#039;)\n&quot;,&quot;ax2.scatter(x[0][np.where(r &lt; Rcrit)], x[1][np.where(r &lt; Rcrit)], s=80, color=&#039;b&#039;)\n&quot;,&quot;ax2.axis(&#039;equal&#039;)\n&quot;,&quot;ax2.grid()\n&quot;,&quot;ax2.set_ylim([-6, 6])\n&quot;,&quot;ax2.set_xlim([-6, 6])\n&quot;,&quot;\n&quot;,&quot;ax3.scatter(x[0][np.where((r &lt; Rcrit )*y)], x[1][np.where((r &lt; Rcrit )*y)], s=80, color=&#039;k&#039;)\n&quot;,&quot;ax3.scatter(x[0][np.where((r &gt; Rcrit )*(1-y))], x[1][np.where((r &gt; Rcrit )*(1-y))], s=80, color=&#039;k&#039;)\n&quot;,&quot;ax3.grid()\n&quot;,&quot;ax3.set_ylim([-6, 6])\n&quot;,&quot;ax3.set_xlim([-6, 6])\n&quot;,&quot;\n&quot;,&quot;acc = 100 - (np.sum((r &lt; Rcrit)*y) + np.sum((r &gt; Rcrit)*(1-y)))/y.shape[0]*100\n&quot;,&quot;print(&#039;With this strategy, an accuracy of %f is obtained&#039; %acc)\n&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;f_krnoFKgkOH&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***3) Neural network initialization in PyTorch***\n&quot;,&quot;\n&quot;,&quot;We can now try to reproduce the previous results with PyTorch. The objective is to get familiar with it, to be used in the next lectures exporing deep learning techniques. First, we need to import PyTorch (`import torch`) and PyTorch Lightning (`import pytorch_lightning`), a high level library that wraps PyTorch.\n&quot;,&quot;\n&quot;,&quot;Here you can find the [**official PyTorch documentation**](<a href="https://pytorch.org/docs/stable/index.html).&quot" class="_blanktarget">https://pytorch.org/docs/stable/index.html).&quot</a>;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;5pFRR8Z8d6wS&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;import torch\n&quot;,&quot;import pytorch_lightning as pl\n&quot;,&quot;torch.manual_seed(42);&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;QTW9nknyeB30&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;PyTorch works with **Tensors**, which can be easily converted to, and from, Numpy Arrays. In fact, most of PyTorch&#039;s syntax follows Numpy own syntax very similarly.\n&quot;,&quot;\n&quot;,&quot;(**WARNING**: but be careful! There are some exceptions, such as np.shape vs torch.size() to retrieve Array or Tensor shape)\n&quot;,&quot;\n&quot;,&quot;Some typical examples can be found here [<a href="https://rickwierenga.com/blog/machine%20learning/numpy-vs-pytorch-linalg.html" class="_blanktarget">https://rickwierenga.com/blog/machine%20learning/numpy-vs-pytorch-linalg.html</a>](<a href="https://rickwierenga.com/blog/machine%20learning/numpy-vs-pytorch-linalg.html)&quot" class="_blanktarget">https://rickwierenga.com/blog/machine%20learning/numpy-vs-pytorch-linalg.html)&quot</a>;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;7wDpzrsXfecK&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;Let&#039;s transform our dataset from Numpy into a PyTorch Tensor:&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;7CkG8rijrsqO&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;# Converte data from numpy to torch\n&quot;,&quot;x_t = torch.from_numpy(x)\n&quot;,&quot;y_t = torch.from_numpy<img class="icon emoticon" alt="Oui" title="Oui" src="https://lms.isae.fr/theme/image.php/fordson/core/1669649644/s/yes" />&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;-MBb_wd3T_1R&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;In case of doubt, the type (array or tensor) can be obtained by using the function `type()`. In the following, the type of the data are printed:&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;h0JiH-Ayh91k&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;# You can see that x is a Numpy Array and x_t is a PyTorch Tensor\n&quot;,&quot;print(type(x))\n&quot;,&quot;print(type(x_t))&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;ZP-7XWn_UTtC&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;And the shape of these data, as an exception, is different for numpy arrays (`shape`) and tensors (`size`):&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;nLyPSoIuiZLh&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;# How to retrieve the shape of Numpy Array/PyTorch Tensor\n&quot;,&quot;print(x.shape)\n&quot;,&quot;print(x_t.size())\n&quot;,&quot;print(y.shape)\n&quot;,&quot;print(y_t.size())&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;UYcHbhKRf6S-&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;As every machine learning problem, the dataset has to be first defined, from which data will be sampled. Let&#039;s use PyTorch class `TensorDataset`, where each sample will be retrieved by indexing tensors along the **first dimension** (called the **batch dimension**). \n&quot;,&quot;\n&quot;,&quot;Therefore, the `x_t` tensor has to be transposed, so that the batch dimension is at the first dimension (index 0 in python).&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;eMiNBGfPfzyF&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;from torch.utils.data import TensorDataset\n&quot;,&quot;# Transpose to set \&quot;batch dimension\&quot; into dim=0, then transform to float type\n&quot;,&quot;train_dataset = TensorDataset(torch.transpose(x_t.float(), 1, 0), y_t.float())&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;loCGuKCdjI9M&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;In PyTorch, a model is defined in two steps, using an **object-oriented** progamming:\n&quot;,&quot;\n&quot;,&quot;1.   An **initialization** function to define all layers with **trainable** parameters (Dense Linear layers, Convolution Layers, etc)\n&quot;,&quot;2.   The function **forward(x)** that will define the precise forward pass of our Neural Network. x is the input of the model, i.e. in this example the coordinates of the points (x1, x2). All the layers with **trainable** parameters **MUST** be defined in the initialization function (e.g. torch&#039;s Linear, ConvXD, RNN etc). The rest (e.g. activation functions can be set there directly).\n&quot;,&quot;\n&quot;,&quot;**Reminder**: We use the `self` keyword to refer to the instance of the class. In practice, the use of `self` gives access to the attributes and methods of the class in Python.\n&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;6DHmb1_MkTcO&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;import torch.nn as nn\n&quot;,&quot;\n&quot;,&quot;def init_layers(self):\n&quot;,&quot;    input_dim = 2\n&quot;,&quot;    self.linear_1 = nn.Linear(input_dim, 2)\n&quot;,&quot;    self.linear_2 = nn.Linear(2, 2)\n&quot;,&quot;    self.linear_3 = nn.Linear(2, 2)\n&quot;,&quot;    self.linear_4 = nn.Linear(2, 1)\n&quot;,&quot;    self.sigmoid = nn.Sigmoid()\n&quot;,&quot;\n&quot;,&quot;def forward(self, x):\n&quot;,&quot;    input_dim = 2 # Input dimension: (x1, x2) coordinates\n&quot;,&quot;    x = self.linear_1(x)\n&quot;,&quot;    x = torch.relu(x)\n&quot;,&quot;    x = self.linear_2(x)\n&quot;,&quot;    x = torch.relu(x)\n&quot;,&quot;    x = self.linear_3(x)\n&quot;,&quot;    x = torch.relu(x)\n&quot;,&quot;    x = self.linear_4(x)\n&quot;,&quot;    x = self.sigmoid(x)\n&quot;,&quot;    return x&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;662ZUw40vQhz&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;The previous model defines a neural network with one input layer (`linear_1`) and one output layer (`linear_4`). For the latter, a **sigmoid activation function** is introduced. Moreover, the network contains also **two hidden layers** (`linear_2` and `linear_3`, each followed by a **ReLu activation function**).\n&quot;,&quot;\n&quot;,&quot;**Note** how the function `forwards` is built: the input `x` actually enters the first intput layer, for which the output is renamed `x`, before entering into the activation function and the next layer. In such a way, the data actually flows through the layers, which is what a **feed forward** neural network does.\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;,&quot;**Question** Why ReLu activation functions are used for the first layers, whereas a sigmoid activation function is employed for the output layer?\n&quot;,&quot;\n&quot;,&quot;Solution is: ...\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;66Brn1374vcp&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***4) Loss function and metrics***\n&quot;,&quot;\n&quot;,&quot;### ***a) The binary cross-entropy loss function***\n&quot;,&quot;As every machine learning, a **loss function** has to be defined, to be used as a cost function during the training phase optimizing the network&#039;s weights. Here, the problem falls into a well popoular type of machine learning problem known as **classifiers**: the objective is to assigne a class (discrete values) to an input. Here, there are two classes, *blue* and *red*. \n&quot;,&quot;\n&quot;,&quot;These classes could be used as outputs of the network, however, no estimation of how confident is the model would be possible: outputting *blue*, either the network is completely right, or completely false. To provide a more grayish prediction, it is often prefered to output a **probability that the class** is blue for example. Consequently, the loss should evaluate how good/bad are the predicted probabilities.\n&quot;,&quot;\n&quot;,&quot;For **binary classification**, the best loss function is the **Binary cross-entropy loss** when the output is in $[0, 1]$. It usually provides the best accuracy, except in specific problems, and is therefore used as default. Note that for multi-class classification or regression problems, other loss functions should be employed. In short, the goal is to predict the probability for a given $x$ to get the correct class $y$ based on the network model $\\hat y = h(x ; {\\bf w})$. For a binary class problem, the output is either $0$ (blue) or $1$ (red), so that the problem is\n&quot;,&quot;\n&quot;,&quot;<span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=P%28%5C%5Chat%20y%20%3D%200%20%7C%20x%29%20%5C%5Capprox%201%20~%5C%5Ctext%7Bif%7D~%20y%3D0" id="action_link6391f2bb1de387" class=""  title="TeX" ><img class="texrender" title="P(\\hat y = 0 | x) \\approx 1 ~\\text{if}~ y=0" alt="P(\\hat y = 0 | x) \\approx 1 ~\\text{if}~ y=0" src="https://lms.isae.fr/filter/tex/pix.php/acc77e0aadee746a358cee9b090d36b0.gif" /></a></span><script type="math/tex">P(\\hat y = 0 | x) \\approx 1 ~\\text{if}~ y=0</script></span>\n&quot;,&quot;<span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=P%28%5C%5Chat%20y%20%3D%201%20%7C%20x%29%20%5C%5Capprox%201%20~%5C%5Ctext%7Bif%7D~%20y%3D1" id="action_link6391f2bb1de388" class=""  title="TeX" ><img class="texrender" title="P(\\hat y = 1 | x) \\approx 1 ~\\text{if}~ y=1" alt="P(\\hat y = 1 | x) \\approx 1 ~\\text{if}~ y=1" src="https://lms.isae.fr/filter/tex/pix.php/f89d0206c3031f8009fb1aaa0443de67.gif" /></a></span><script type="math/tex">P(\\hat y = 1 | x) \\approx 1 ~\\text{if}~ y=1</script></span>\n&quot;,&quot;\n&quot;,&quot;which can be written in a compact form\n&quot;,&quot;\n&quot;,&quot;<span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=%20P%28y%20%7C%20x%29%20%3D%20h%28x%3B%20%7B%5C%5Cbf%20w%7D%29%5Ey%20%5B1-h%28x%3B%20%7B%5C%5Cbf%20w%7D%29%5D%5E%7B1-y%7D%20" id="action_link6391f2bb1de389" class=""  title="TeX" ><img class="texrender" title=" P(y | x) = h(x; {\\bf w})^y [1-h(x; {\\bf w})]^{1-y} " alt=" P(y | x) = h(x; {\\bf w})^y [1-h(x; {\\bf w})]^{1-y} " src="https://lms.isae.fr/filter/tex/pix.php/ae980986f89cf1e484050b2114930d49.gif" /></a></span><script type="math/tex"> P(y | x) = h(x; {\\bf w})^y [1-h(x; {\\bf w})]^{1-y} </script></span>\n&quot;,&quot;\n&quot;,&quot;This can be achieved by maximizing the likelihood for all training samples indexed by $i$:\n&quot;,&quot;\n&quot;,&quot;<span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=%20%5C%5Cmathcal%7BL%7D%28%7B%5C%5Cbf%20w%7D%29%20%3D%20P%28y_1%2C%20.%2C%20y_i%2C%20.y_N%20%7C%20x_1%2C%20.%2C%20x_i%2C%20.%20x_N%29%20%3D%20%5C%5Cprod_%7Bi%3D1%7D%5EN%20P%28%5C%5Chat%20y_i%20%7C%20x%20%3B%20%7B%5C%5Cbf%20w%7D%29%20%20%3D%20%5C%5Cprod_%7Bi%3D1%7D%5EN%20h%28x_i%29%5E%7By_i%7D%20%5B1-h%28x_i%29%5D%5E%7B1-y_i%7D%20%20" id="action_link6391f2bb1de3810" class=""  title="TeX" ><img class="texrender" title=" \\mathcal{L}({\\bf w}) = P(y_1, ., y_i, .y_N | x_1, ., x_i, . x_N) = \\prod_{i=1}^N P(\\hat y_i | x ; {\\bf w})  = \\prod_{i=1}^N h(x_i)^{y_i} [1-h(x_i)]^{1-y_i}  " alt=" \\mathcal{L}({\\bf w}) = P(y_1, ., y_i, .y_N | x_1, ., x_i, . x_N) = \\prod_{i=1}^N P(\\hat y_i | x ; {\\bf w})  = \\prod_{i=1}^N h(x_i)^{y_i} [1-h(x_i)]^{1-y_i}  " src="https://lms.isae.fr/filter/tex/pix.php/4f98f8fe6147e16e84c86296d4e635f4.gif" /></a></span><script type="math/tex"> \\mathcal{L}({\\bf w}) = P(y_1, ., y_i, .y_N | x_1, ., x_i, . x_N) = \\prod_{i=1}^N P(\\hat y_i | x ; {\\bf w})  = \\prod_{i=1}^N h(x_i)^{y_i} [1-h(x_i)]^{1-y_i}  </script></span>\n&quot;,&quot;\n&quot;,&quot;Taking the negative (to be minimized) log-likelihood (for practical and stability reasons) normalized by the number of samples (for normalization purposes), provides the **Binary Cross-Entropy (BCE)** loss function:\n&quot;,&quot;\n&quot;,&quot;<span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=%5C%5Ctext%7BBCE%7D%28%7B%5C%5Cbf%20w%7D%29%20%3D%20%5C%5Cfrac%7B1%7D%7BN%7D%5C%5Csum_%7Bi%3D1%5EN%7D%20%20y_i%20log%5Bh%28x%3B%20%7B%5C%5Cbf%20w%7D%29%5D%20%2B%20%281-y_i%29%20log%5B1-h%28x%3B%20%7B%5C%5Cbf%20w%7D%29%5D" id="action_link6391f2bb1de3811" class=""  title="TeX" ><img class="texrender" title="\\text{BCE}({\\bf w}) = \\frac{1}{N}\\sum_{i=1^N}  y_i log[h(x; {\\bf w})] + (1-y_i) log[1-h(x; {\\bf w})]" alt="\\text{BCE}({\\bf w}) = \\frac{1}{N}\\sum_{i=1^N}  y_i log[h(x; {\\bf w})] + (1-y_i) log[1-h(x; {\\bf w})]" src="https://lms.isae.fr/filter/tex/pix.php/e6a92d7a5b101ee2a2742848910304b3.gif" /></a></span><script type="math/tex">\\text{BCE}({\\bf w}) = \\frac{1}{N}\\sum_{i=1^N}  y_i log[h(x; {\\bf w})] + (1-y_i) log[1-h(x; {\\bf w})]</script></span>\n&quot;,&quot;\n&quot;,&quot;This loss function can be used in PyTorch using `binary_cross_entropy` for binary classifiers. The name *binary* is because it is restricted to two-class problems (compared with multi-class problems), and *cross-entropy* because of its link with entropy distribution. \n&quot;,&quot;\n&quot;,&quot;As a brief explanation, for a distribution $p<img class="icon emoticon" alt="Oui" title="Oui" src="https://lms.isae.fr/theme/image.php/fordson/core/1669649644/s/yes" />$, the **entropy** is given by\n&quot;,&quot;\n&quot;,&quot;<span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=H%28p%29%20%3D%20-%20%5C%5Csum_%7Bi%3D1%7D%5EN%20p%28y_i%29%20log%5Bp%28y_i%29%5D" id="action_link6391f2bb1de3812" class=""  title="TeX" ><img class="texrender" title="H(p) = - \\sum_{i=1}^N p(y_i) log[p(y_i)]" alt="H(p) = - \\sum_{i=1}^N p(y_i) log[p(y_i)]" src="https://lms.isae.fr/filter/tex/pix.php/37f9cbc46615d987c734f94ffe504948.gif" /></a></span><script type="math/tex">H(p) = - \\sum_{i=1}^N p(y_i) log[p(y_i)]</script></span>\n&quot;,&quot;\n&quot;,&quot;which measures the *uncertainty* associated with a distribution. The same can be evaluated for a probability $p<img class="icon emoticon" alt="Oui" title="Oui" src="https://lms.isae.fr/theme/image.php/fordson/core/1669649644/s/yes" />$ compared with a reference distribution $q<img class="icon emoticon" alt="Oui" title="Oui" src="https://lms.isae.fr/theme/image.php/fordson/core/1669649644/s/yes" />$: this is the **cross-entropy**, written as\n&quot;,&quot;\n&quot;,&quot;<span class="nolink"><span class="MathJax_Preview"><a href="https://lms.isae.fr/filter/tex/displaytex.php?texexp=H%28p%2C%20q%29%20%3D%20-%20%5C%5Csum_%7Bi%3D1%7D%5EN%20q%28y_i%29%20log%5Bp%28y_i%29%5D" id="action_link6391f2bb1de3813" class=""  title="TeX" ><img class="texrender" title="H(p, q) = - \\sum_{i=1}^N q(y_i) log[p(y_i)]" alt="H(p, q) = - \\sum_{i=1}^N q(y_i) log[p(y_i)]" src="https://lms.isae.fr/filter/tex/pix.php/a8b29a111bd55f023baa144627924eaa.gif" /></a></span><script type="math/tex">H(p, q) = - \\sum_{i=1}^N q(y_i) log[p(y_i)]</script></span>\n&quot;,&quot;\n&quot;,&quot;This cross-entropy is also associated with the more general **Kullback-Liebler divergence**, also used as loss function in multi-class classifiers, which measures how close a probabiliy $p$ (the network&#039;s prediction) is from a reference $q$ (the ground truth distribution). \n&quot;,&quot;\n&quot;,&quot;### ***b) The accuracy metric***\n&quot;,&quot;\n&quot;,&quot;In addition to the loss function, a **metric** can be defined to **evaluate the performance of the neural network**. This metric is NOT used to train (i.e. to optimize the weights of the neural network), but only to display a user-friendly value to assess the network performance: it allows the interpretation  much easier than the loss function. \n&quot;,&quot;For instance, how one can juge if a classifier with a loss of $10^{-3}$ is good or not? How, if the associated metric **accuracy** is $98\\%$: most of the time the network predictions will be considered as satisfying. Note that a random guess of the class, considering there are only two classes possible, would lead to an accuracy of $50\\%$: a network model should at least reach better accuracy than $50\\%$, a threshold impossible to evaluate with a loss function.\n&quot;,&quot;\n&quot;,&quot;The `accuracy` is defined in the code below:&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;kDVnn3bq4nNh&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;def accuracy(y, y_hat):\n&quot;,&quot;    pred = y_hat.view(1, -1) &gt;= 0.5\n&quot;,&quot;    truth = y.view(1, -1) &gt;= 0.5\n&quot;,&quot;    return (pred == truth).float().sum() / len<img class="icon emoticon" alt="Oui" title="Oui" src="https://lms.isae.fr/theme/image.php/fordson/core/1669649644/s/yes" />&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;rWJJ2JKXmIPS&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***5) Network setup***\n&quot;,&quot;\n&quot;,&quot;Finally, the neural network model is defined in PyTorch using the following arguments:\n&quot;,&quot;\n&quot;,&quot;1.   Batch size (`batch_size=16`)\n&quot;,&quot;2.   Learning rate (`learning_rate=1.0e-3`)\n&quot;,&quot;3.   Weight decay (`weight_decay=0.00001`)\n&quot;,&quot;4.   Train dataset (defined in III-2, named `train_dataset`)\n&quot;,&quot;5.   Optimizer (here, a well nown optimizer called `Adam` is used)\n&quot;,&quot;7.   Loss function (described in III-4)\n&quot;,&quot;8.   Metrics (if any, here defined in III-4)\n&quot;,&quot;6.   Function to initialize Neural network layers (already defined in III-3)\n&quot;,&quot;7.   Forward function (to built the neural netrwork, already defined in III-3).\n&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;7V8DaTiAQ38S&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;from torch.nn import functional as F\n&quot;,&quot;\n&quot;,&quot;model = tp1.MLP(batch_size=16, learning_rate=1.0e-3, weight_decay=0.00001,\n&quot;,&quot;                  train_dataset=train_dataset,\n&quot;,&quot;                  train_optim=torch.optim.Adam,\n&quot;,&quot;                  loss=F.binary_cross_entropy,\n&quot;,&quot;                  metrics={&#039;accuracy&#039; : accuracy},\n&quot;,&quot;                  init_layers=init_layers,\n&quot;,&quot;                  forward=forward)&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;f5YXtv62Jimc&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;Some comments on the previous code snippet:\n&quot;,&quot;\n&quot;,&quot;We have defined the `model` object, which is an instantiation of the `MLP` class.\n&quot;,&quot;\n&quot;,&quot;Some optimization algorithms are already implemented in PyTorch, in the `torch.optim` module. You can find more details in the [**official documentation**](<a href="https://pytorch.org/docs/stable/optim.html" class="_blanktarget">https://pytorch.org/docs/stable/optim.html</a>).\n&quot;,&quot;\n&quot;,&quot;The same applies for loss functions, defined in the `torch.nn.functional` and `torch.nn` modules. The difference is that `torch.nn.functional` provides functions for direct evaluation while `torch.nn` implements classes that must be first instatiated. Their use is very similar, and depends on the context. Again, you can find examples of classification and regression losses in the [**PyTorch docs**](<a href="https://pytorch.org/docs/stable/nn.functional.html#loss-functions" class="_blanktarget">https://pytorch.org/docs/stable/nn.functional.html#loss-functions</a>).\n&quot;,&quot;\n&quot;,&quot;In the MLP class, the loss argument can only receive **one loss**, i.e. when combining several losses you should write your custom loss function that outputs a single scalar value corresponding to the optimization loss.\n&quot;,&quot;\n&quot;,&quot;On the contrary, you can set **several metrics** to monitor your training in an user-friendly way. This is done by passing to the `metrics` arguments a python dictionary. It should contain tha names of the metric and the function defining those:\n&quot;,&quot;```\n&quot;,&quot;    metrics = {&#039;myMetricName&#039; : functionDefiningMyMetric}\n&quot;,&quot;```\n&quot;,&quot;\n&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;fWa1dTYdPChu&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***6) Training the Network***\n&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;oXEMrCPhntSq&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;Finally, this network model can be trained, either on a cpu (no extra arguments required, or `gpus=0`) or on a gpu (using `gpus=1`). Note that special methods such as `.cuda()` or `.cpu()` can be used to transfer/convert model or inputs from one device to another.\n&quot;,&quot;\n&quot;,&quot;Finally to train the network, we need to specify the number of `epoch` desired. &quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;B3JXd27qH8l4&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;Use_gpu = 0\n&quot;,&quot;trainer = pl.Trainer(max_epochs=50, gpus=Use_gpu);\n&quot;,&quot;import time\n&quot;,&quot;start_time = time.time()\n&quot;,&quot;trainer.fit(model)\n&quot;,&quot;print(\&quot;--- %s seconds ---\&quot; % (time.time() - start_time))&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;Wq5NLUkT1sEL&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;Now that once the neural network is trained, the loss and metric (accuracy) during the training phase can be displayed. In PyTorch, this is contains in the `history` of the trained model (this is implemented in our custom `MLP` class).&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;zqPAndq0i2t7&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;history = model.history\n&quot;,&quot;\n&quot;,&quot;fig, ax1 = plt.subplots()\n&quot;,&quot;\n&quot;,&quot;color = &#039;tab:red&#039;\n&quot;,&quot;ax1.set_xlabel(&#039;Epochs&#039;)\n&quot;,&quot;ax1.set_ylabel(&#039;Accuracy&#039;, color=color)\n&quot;,&quot;ax1.plot(history[&#039;accuracy&#039;], color=color)\n&quot;,&quot;ax1.tick_params(axis=&#039;y&#039;, labelcolor=color)\n&quot;,&quot;\n&quot;,&quot;ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n&quot;,&quot;\n&quot;,&quot;color = &#039;tab:blue&#039;\n&quot;,&quot;ax2.set_ylabel(&#039;Loss&#039;, color=color)  # we already handled the x-label with ax1\n&quot;,&quot;ax2.plot(history[&#039;loss&#039;], color=color)\n&quot;,&quot;ax2.tick_params(axis=&#039;y&#039;, labelcolor=color)\n&quot;,&quot;\n&quot;,&quot;fig.tight_layout()  # otherwise the right y-label is slightly clipped&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;n-4qE8jg1yQg&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;**QUESTIONS**\n&quot;,&quot;\n&quot;,&quot;- Describe the network training phase. What happens for low number of epochs? How it evolves with larger epochs?\n&quot;,&quot;\n&quot;,&quot;- Determine the accuracy of your model? What is the probability that your model gives the correct answer if a new sample is generated?\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;l1MMEMK_zsXf&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;## ***6) Network validation***\n&quot;,&quot;\n&quot;,&quot;So far, the network has been optimized on the training dataset. No guarantee can be given for the performance on unseen data: optimizing too large networks on too small dataset could lead to **overfitting**. A way to ensure that the network is actually **learning rather than memorizing** is the validation/test phase. \n&quot;,&quot;\n&quot;,&quot;To do so, new samples are generated for the same problem, but not used for training. Then the network&#039;s predictions are tested on those unseen data: a proper network training should give similar loss/metric values for both the training and validation datasets.\n&quot;,&quot;\n&quot;,&quot;The following code propose to evaluate the accuracy on the previous trained networl model. First, new samples should be generated.&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;9ytMjFw_I2Lo&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;x_test, y_test = generateData(100)\n&quot;,&quot;print(x_test.shape)\n&quot;,&quot;x_test_t = torch.from_numpy(x_test)\n&quot;,&quot;y_t_test = torch.from_numpy(y_test)\n&quot;,&quot;test_dataset = TensorDataset(torch.transpose(x_test_t.float(), 1, 0), y_t_test.float())&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;VXo8XTz9jqwB&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;Add the test_dataset to the model (*N.B*: Again, we do it in an object-oriented way, through a setter method).&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;hOyNU6wb7n-D&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;model.set_test_dataloader(test_dataset)\n&quot;,&quot;print(model)&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;LouJQ0lh0_zk&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;And let&#039;s run the test&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;V81lqamv9jaY&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;trainer.test(model)\n&quot;,&quot;x_test_tf = x_test_t.float()\n&quot;,&quot;output_n = np.zeros((200, 1))\n&quot;,&quot;\n&quot;,&quot;device = &#039;cpu&#039;\n&quot;,&quot;if (torch.cuda.is_available() and Use_gpu != 0):\n&quot;,&quot;  device = &#039;cuda&#039;\n&quot;,&quot;  print(&#039;Cuda is used for GPUs&#039;)\n&quot;,&quot;\n&quot;,&quot;with torch.no_grad():\n&quot;,&quot;  for i in range(200):\n&quot;,&quot;    output = model.forward(x_test_tf[:, i].to(device))\n&quot;,&quot;    output_n[i, 0] = output.cpu().numpy()\n&quot;,&quot;  x_test_n = x_test_tf.cpu().numpy()\n&quot;,&quot;  #print(x_test_n)\n&quot;,&quot;  #print(output_n)\n&quot;,&quot;avg_test_acc = (sum(model.history[&#039;accuracy&#039;])/len(model.history[&#039;accuracy&#039;])) * 100\n&quot;,&quot;print(f&#039;Avg. test accuracy {avg_test_acc:.3f}&#039;)\n&quot;,&quot;\n&quot;,&quot;fig, ax1 = plt.subplots()\n&quot;,&quot;\n&quot;,&quot;color = &#039;tab:red&#039;\n&quot;,&quot;ax1.set_xlabel(&#039;Tests&#039;)\n&quot;,&quot;ax1.set_ylabel(&#039;Accuracy&#039;, color=color)\n&quot;,&quot;ax1.plot(model.history[&#039;accuracy&#039;], color=color)\n&quot;,&quot;ax1.tick_params(axis=&#039;y&#039;, labelcolor=color)\n&quot;,&quot;\n&quot;,&quot;fig.tight_layout()  # otherwise the right y-label is slightly clipped\n&quot;,&quot;\n&quot;,&quot;fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n&quot;,&quot;\n&quot;,&quot;ax1.scatter(x_test_t[0][np.where(y_t_test == 1)], x_test_t[1][np.where(y_t_test == 1)], s=80, color=&#039;r&#039;)\n&quot;,&quot;ax1.scatter(x_test_t[0][np.where(y_t_test == 0)], x_test_t[1][np.where(y_t_test == 0)], s=80, color=&#039;b&#039;)\n&quot;,&quot;ax1.axis(&#039;equal&#039;)\n&quot;,&quot;ax1.grid()\n&quot;,&quot;ax1.set_ylim([-6, 6])\n&quot;,&quot;ax1.set_xlim([-6, 6])\n&quot;,&quot;ax1.set_title(&#039;Target&#039;)\n&quot;,&quot;\n&quot;,&quot;red_cat = ax2.scatter(x_test_n[0, :][np.where(output_n[:,0] &gt;= 0.5)], x_test_n[1, :][np.where(output_n[:,0] &gt;= 0.5)], s=80, color=&#039;r&#039;)\n&quot;,&quot;blue_cat = ax2.scatter(x_test_n[0, :][np.where(output_n[:,0] &lt;= 0.5)], x_test_n[1, :][np.where(output_n[:,0] &lt;= 0.5)], s=80, color=&#039;b&#039;)\n&quot;,&quot;# Check the values where the MLP is not confident, here set to the probability P &gt; 0.25 and P &lt; 0.75)\n&quot;,&quot;unsure_cat = ax2.scatter(x_test_n[0, :][np.where( np.abs(output_n[:,0]-0.5) &lt;= 0.25 )], x_test_n[1, :][np.where( np.abs(output_n[:,0]-0.5) &lt;= 0.25 )], s=80, color=&#039;k&#039;, marker=&#039;x&#039;)\n&quot;,&quot;ax2.axis(&#039;equal&#039;)\n&quot;,&quot;ax2.grid()\n&quot;,&quot;ax2.set_ylim([-6, 6])\n&quot;,&quot;ax2.set_xlim([-6, 6])\n&quot;,&quot;ax2.set_title(&#039;MLP predictions&#039;)\n&quot;,&quot;ax2.legend((red_cat, blue_cat, unsure_cat), (&#039;p &gt; 0.5&#039;, &#039;p &lt; 0.5&#039;, &#039;0.25 &lt; p &lt; 0.75&#039;))\n&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;R6jvSb1rj9og&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;**EXERCISE 8**\n&quot;,&quot;\n&quot;,&quot;**8a)** Try now to reproduce the work done on playground in the section II.1) and II.2) with the Pytorch API.\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;\n&quot;,&quot;---\n&quot;,&quot;\n&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;NHhbJxl-jpbj&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;# IV - Garonne challenge&quot;]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;EVwCAQr2jx5b&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;Now, it&#039;s your turn to try to implement a neural network to tackle your Garonne challenge.\n&quot;,&quot;Some help is given regarding the pre-processing of data, but you should be able to create an MLP on such data.&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;YgRHhTh22gCZ&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;! pip install git+<a href="https://github.com/PytorchLightning/pytorch-lightning.git@master" class="_blanktarget">https://github.com/PytorchLightning/pytorch-lightning.git@master</a> --upgrade&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;MwxfTTkG2lsL&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;! FOLDER=\&quot;machine_learning_isae_1a\&quot;; URL=\&quot;<a href="https://gitlab.com/a.alguacil-cabrerizo/machine_learning_isae_1a" class="_blanktarget">https://gitlab.com/a.alguacil-cabrerizo/machine_learning_isae_1a</a>\&quot;; if [ ! -d \&quot;$FOLDER\&quot; ] ; then git clone $URL $FOLDER ; else cd \&quot;$FOLDER\&quot; &amp;&amp; git pull; fi&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;_wgbZVgY7SXU&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;import torch\n&quot;,&quot;import pytorch_lightning as pl\n&quot;,&quot;torch.manual_seed(42);&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;SMlmvlCj2pEg&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;import importlib\n&quot;,&quot;import os\n&quot;,&quot;PROJECT_PATH = \&quot;machine_learning_isae_1a/dl/tp1\&quot;\n&quot;,&quot;\n&quot;,&quot;spec = importlib.util.spec_from_file_location(&#039;tp1&#039;, os.path.join(PROJECT_PATH, &#039;mlp.py&#039;))\n&quot;,&quot;tp1 = importlib.util.module_from_spec(spec)\n&quot;,&quot;spec.loader.exec_module(tp1)&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;bSp7XKwDjr93&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;!wget -nv <a href="https://cerfacs.fr/opendata/cours/data.tar.gz" class="_blanktarget">https://cerfacs.fr/opendata/cours/data.tar.gz</a>\n&quot;,&quot;!tar -xzf data.tar.gz\n&quot;,&quot;!rm data.tar.gz\n&quot;,&quot;\n&quot;,&quot;import numpy as np\n&quot;,&quot;import pandas as pd\n&quot;,&quot;from sklearn.preprocessing import StandardScaler\n&quot;,&quot;\n&quot;,&quot;# Read the data contained in the .scv files\n&quot;,&quot;q_ton = pd.read_csv(\&quot;data/q_Tonneins_1998-2004.csv\&quot;, parse_dates=True, index_col = 0)\n&quot;,&quot;h_lar = pd.read_csv(\&quot;data/h_Lareole_1998-2004.csv\&quot;, parse_dates=True, index_col = 0)\n&quot;,&quot;mascaret_output = pd.read_csv(\&quot;data/mascaret_output_1998-2004.csv\&quot;, parse_dates=True, index_col = 0)\n&quot;,&quot;\n&quot;,&quot;# Interpolation on the same timeframe\n&quot;,&quot;q_ton = q_ton.resample(&#039;H&#039;).mean().interpolate(&#039;linear&#039;)\n&quot;,&quot;h_lar = h_lar.resample(&#039;H&#039;).mean().interpolate(&#039;linear&#039;)\n&quot;,&quot;\n&quot;,&quot;interp_q_ton = q_ton.resample(&#039;D&#039;).interpolate(&#039;linear&#039;)\n&quot;,&quot;interp_h_lar = h_lar.resample(&#039;D&#039;).interpolate(&#039;linear&#039;)\n&quot;,&quot;\n&quot;,&quot;#We concatenate the two inputs features in the same dataframe\n&quot;,&quot;inputs = pd.concat((interp_q_ton, interp_h_lar), axis=1)\n&quot;,&quot;outputs = mascaret_output\n&quot;,&quot;\n&quot;,&quot;#Separate the training and test set\n&quot;,&quot;inputs_train, inputs_test = inputs[&#039;1998&#039;:&#039;2002&#039;], inputs[&#039;2003&#039;:&#039;2004&#039;]\n&quot;,&quot;y_train, y_test = outputs[&#039;1998&#039;:&#039;2002&#039;], outputs[&#039;2003&#039;:&#039;2004&#039;]\n&quot;,&quot;\n&quot;,&quot;print(&#039;Data are correctly read and interpolated by Pandas&#039;)&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;crf003Elaibh&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;from torch.utils.data import TensorDataset\n&quot;,&quot;\n&quot;,&quot;X_train_h, X_train_q = inputs_train[&#039;height&#039;][:, np.newaxis], inputs_train[&#039;Flow&#039;][:, np.newaxis]\n&quot;,&quot;X_test_h, X_test_q = inputs_test[&#039;height&#039;][:, np.newaxis], inputs_test[&#039;Flow&#039;][:, np.newaxis]\n&quot;,&quot;\n&quot;,&quot;X_train_h_t_1d = torch.from_numpy(X_train_h)\n&quot;,&quot;X_train_h_q_1d = torch.from_numpy(X_train_q)\n&quot;,&quot;X_test_h_t_1d = torch.from_numpy(X_test_h)\n&quot;,&quot;X_test_h_q_1d = torch.from_numpy(X_test_q)&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;fWkjEVOJG6gX&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;window = 83\n&quot;,&quot;X_train_h_t = X_train_h_t_1d.reshape(-1, window)\n&quot;,&quot;X_train_h_q = X_train_h_q_1d.reshape(-1, window)\n&quot;,&quot;\n&quot;,&quot;crop_size = int(np.floor( X_test_h_t_1d.size(0) / window ))\n&quot;,&quot;X_test_h_t = X_test_h_t_1d[:crop_size*window].reshape(-1, window)\n&quot;,&quot;X_test_h_q = X_test_h_q_1d[:crop_size*window].reshape(-1, window)&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;0cmDW3kyHPbD&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;%matplotlib inline\n&quot;,&quot;import matplotlib.pyplot as plt\n&quot;,&quot;from scipy import stats\n&quot;,&quot;import numpy as np\n&quot;,&quot;\n&quot;,&quot;print(&#039;Height at La Reole&#039;)\n&quot;,&quot;print(stats.describe(X_train_h_t.reshape(-1)))\n&quot;,&quot;print()\n&quot;,&quot;\n&quot;,&quot;print(&#039;Flow at Tonneins&#039;)\n&quot;,&quot;print(stats.describe(X_train_h_q.reshape(-1)))\n&quot;,&quot;print()\n&quot;,&quot;\n&quot;,&quot;f, ax1 = plt.subplots()\n&quot;,&quot;color = &#039;tab:red&#039;\n&quot;,&quot;ax1.hist(X_train_h_t.reshape(-1), density=False, alpha=0.5, label=&#039;H&#039;, color=color)\n&quot;,&quot;ax1.set_ylabel(&#039;Frequency&#039;)\n&quot;,&quot;ax1.set_xlabel(&#039;Value&#039;)\n&quot;,&quot;#ax2 = ax1.twinx()\n&quot;,&quot;color = &#039;tab:blue&#039;\n&quot;,&quot;ax1.hist(X_train_h_q.reshape(-1), density=False, alpha=0.8, label=&#039;Q&#039;, color=color)\n&quot;,&quot;#ax2.tick_params(axis=&#039;y&#039;, labelcolor=color)\n&quot;,&quot;#ax2.set_ylabel(&#039;Frequency&#039;, color=color)\n&quot;,&quot;f.gca().set_xscale(\&quot;log\&quot;)\n&quot;,&quot;f.legend(loc=&#039;upper right&#039;)&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;U1QNkx9MTBrY&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;X_train_h_t_norm = (X_train_h_t - X_train_h_t.mean()) / X_train_h_t.std()\n&quot;,&quot;X_train_h_q_norm = (X_train_h_q - X_train_h_q.mean()) / X_train_h_q.std()\n&quot;,&quot;\n&quot;,&quot;f, ax1 = plt.subplots()\n&quot;,&quot;color = &#039;tab:red&#039;\n&quot;,&quot;ax1.hist(X_train_h_t_norm.reshape(-1), density=False, alpha=0.5, label=&#039;H&#039;, color=color)\n&quot;,&quot;ax1.set_ylabel(&#039;Frequency&#039;)\n&quot;,&quot;ax1.set_xlabel(&#039;Value&#039;)\n&quot;,&quot;#ax2 = ax1.twinx()\n&quot;,&quot;color = &#039;tab:blue&#039;\n&quot;,&quot;ax1.hist(X_train_h_q_norm.reshape(-1), density=False, alpha=0.8, label=&#039;Q&#039;, color=color)\n&quot;,&quot;#ax2.tick_params(axis=&#039;y&#039;, labelcolor=color)\n&quot;,&quot;#ax2.set_ylabel(&#039;Frequency&#039;, color=color)\n&quot;,&quot;f.legend(loc=&#039;upper right&#039;)&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;dS44oCLKS636&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;train_dataset = TensorDataset(X_train_h_t_norm.float(), X_train_h_q_norm.float())&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;i87XhXe6cMjV&quot;,&quot;colab_type&quot;:&quot;text&quot;},&quot;source&quot;:[&quot;Model definition, as in **III.3**:&quot;]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;0CT8WjgecL6V&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;import torch.nn as nn\n&quot;,&quot;\n&quot;,&quot;def init_layers(self):\n&quot;,&quot;    input_dim = 83\n&quot;,&quot;    self.linear_1 = nn.Linear(input_dim, 100)\n&quot;,&quot;    self.linear_2 = nn.Linear(100, 10)\n&quot;,&quot;    self.linear_3 = nn.Linear(10, 50)\n&quot;,&quot;    self.linear_4 = nn.Linear(50, 83)\n&quot;,&quot;\n&quot;,&quot;def forward(self, x):\n&quot;,&quot;    x = self.linear_1(x)\n&quot;,&quot;    x = torch.relu(x)\n&quot;,&quot;    x = self.linear_2(x)\n&quot;,&quot;    x = torch.relu(x)\n&quot;,&quot;    x = self.linear_3(x)\n&quot;,&quot;    x = torch.relu(x)\n&quot;,&quot;    x = self.linear_4(x)\n&quot;,&quot;    return x&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;tvO98_wTAZFt&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;def R2Score(y_pred, y_target):\n&quot;,&quot;  return -torch.var(y_target - y_pred)/torch.var(y_target) + 1.&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;lRIHobRzdBwW&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;from torch.nn import functional as F\n&quot;,&quot;\n&quot;,&quot;model_Garonne = tp1.MLP(batch_size=2, learning_rate=1.0e-4, weight_decay=0.00001,\n&quot;,&quot;                  train_dataset=train_dataset,\n&quot;,&quot;                  train_optim=torch.optim.Adam,\n&quot;,&quot;                  loss=F.mse_loss,\n&quot;,&quot;                  metrics={&#039;R2&#039; : R2Score},\n&quot;,&quot;                  init_layers=init_layers,\n&quot;,&quot;                  forward=forward)&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;0PNwemTINClO&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;print(model_Garonne)&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;2JIU1J0sdVI5&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;trainer = pl.Trainer(max_epochs=200, gpus=1);\n&quot;,&quot;import time\n&quot;,&quot;start_time = time.time()\n&quot;,&quot;trainer.fit(model_Garonne)\n&quot;,&quot;print(\&quot;--- %s seconds ---\&quot; % (time.time() - start_time))&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]},{&quot;cell_type&quot;:&quot;code&quot;,&quot;metadata&quot;:{&quot;id&quot;:&quot;Ym2UnWv42-EE&quot;,&quot;colab_type&quot;:&quot;code&quot;,&quot;colab&quot;:{}},&quot;source&quot;:[&quot;history = model_Garonne.history\n&quot;,&quot;\n&quot;,&quot;fig, ax1 = plt.subplots()\n&quot;,&quot;\n&quot;,&quot;color = &#039;tab:red&#039;\n&quot;,&quot;ax1.set_xlabel(&#039;Epochs&#039;)\n&quot;,&quot;ax1.set_ylabel(&#039;R2&#039;, color=color)\n&quot;,&quot;ax1.plot(history[&#039;R2&#039;], color=color)\n&quot;,&quot;ax1.set_ylim([-1, 1])\n&quot;,&quot;ax1.tick_params(axis=&#039;y&#039;, labelcolor=color)\n&quot;,&quot;\n&quot;,&quot;ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n&quot;,&quot;\n&quot;,&quot;color = &#039;tab:blue&#039;\n&quot;,&quot;ax2.set_ylabel(&#039;Loss&#039;, color=color)  # we already handled the x-label with ax1\n&quot;,&quot;ax2.plot(history[&#039;loss&#039;], color=color)\n&quot;,&quot;ax2.tick_params(axis=&#039;y&#039;, labelcolor=color)\n&quot;,&quot;\n&quot;,&quot;fig.tight_layout()  # otherwise the right y-label is slightly clipped&quot;],&quot;execution_count&quot;:0,&quot;outputs&quot;:[]}]}</div></span> </pre>